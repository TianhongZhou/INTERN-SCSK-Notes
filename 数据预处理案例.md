# 数据预处理案例

## 1. Python机器学习实战案例 -- 集装箱危险品瞒报预测

### 数据集成

- 根据订舱号这一订舱的唯一标识将表里数据集成

### 数据清洗

- 处理缺失值

  ```python
  # 统计缺失值
  def missingDataStat(data):
  	total = data.isnull().sum().sort_values(ascending=False)
      percent = (data.isnull().sum()/data.isnull().count()).sort_values(...)
      missing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])
  ```

  ```python
  # 用KNN填补缺失值
  def missingData(data):
  	sub_data = data.loc[:,['SH_COOP_FREQ','FW_COOP_FREQ','CN_COOP_FREQ']]
  	return KNN(k=3).fit_transform(data)
  ```

- 箱图处理错误数据与异常值

- 处理重复数据 -- 删除多余记录

### 数据变换

- 对于危险品瞒报数据进行转换，得出新的瞒报标志布尔数据
- 引入新相关变量

### 数据离散化

- 数据中部分属性为连续值，数据分布较为分散，对其进行离散化后便于分析，助于模型的稳定，降低过拟合的风险

- ```python
  # 基于K-means算法的聚类离散法
  def coopLvlDiscretization(data, fieldNme, newFieldNme, k):
      fieldData = data[fieldNme].copy()
      # K-means算法
      kmodel = KMeans(n_clusters = k)
      kmodel.fit(fieldData.values.reshape((len(fieldData), 1)))
      kCenter = pd.DataFrame(kmodel.cluster_centers_, columns=list('a'))
      kCenter = kCenter.sort_values(by='a')
      # 确定分类边界
      kBorder = kCenter.rolling(2).mean().iloc[1:]
      kBorder = [0] + list(kBorder.values[:,0]) + [fieldData.max()]
      # 切分数据，实现离散化
      newFieldData = pd.cut(fieldData, kBorder, labels=range(k))
      # 合并添加新列
      data = pd.concat([data, newFieldData.rename(newFieldNme)], axis=1)
      return data
  ```

### 特征重要性筛选

- 采用基于逻辑回归的稳定性选择方法实现对特征的筛选

  ```python
  def featureSelection(data):
      A1 = data[['AGMT_ID', 'ISEBOOKING', 'OOCL_CMDTY_GRP_CDE', 'SH_COOP_FREQ', 'FW_COOP_FREQ', 'CN_COOP_FREQ']]
      B1 = data[['IS_CONCEAL']]
      X1 = A1.values
      y1 = B1.values
      X1[:, 0] = leAgmt.transform(X1[:, 0])
      x1[:, 1] = leEB.transform(X1[:, 1])
      X1[:, 2] = leGrp.transform(X1[:, 2])
      X1[:, 3] = leSH.transform(X1[:, 3])
      X1[:, 4] = leFW.transform(X1[:, 4])
      X1[:, 5] = leCN.transform(X1[:, 5])
      y1 = LabelEncoder().fit_transform(y1.ravel())
      x_train, x_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=0)
      xgboost = XGBClassifier()
      xgboost.fit(x_train, y_train)
      print(xgboost.feature_importances)
      plt.bar(range(len(xgboost.feature_importances_)), xgboost.feature_importance_)
      plt.show()
  ```

### 数据平衡

- 瞒报记录与非瞒报记录占比为1：121，属于严重的数据不平衡

- 采用SMOTE算法，根据相邻样本数据合成新的样本，以补充小类数据不足

- ```python
  def smoteData(data):
  	# 分为训练集和测试集7：3
      A, A2, B, B2 = train_test_split(data[[{feature_columns}]], data[[{label_columns}]], test_size=0.3)
      X = A.values
      y = B.values
      X2 = A2.values
      y2 = B2.values
      # 数据平衡（训练集）
      over_samples = SMOTEENN()
      X, y = over_samples.fit_sample(X, y)
      train = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))), columns=data.columns)
      test = pd.concat([A2, B2], axis=1)
      return [train, test]
  ```



## 2. Python机器学习实战案例 -- 保险产品推荐

### 数据探索

- 对缺失值进行统计，并将缺失值前10位的特征打印

  ```python
  nan_count = data.isnull().sum().sort_values(ascending=False)
  nan_ratio = nan_count / len(data)
  nan_data = pd.concat([nan_count, nan_ratio], axis=1, keys=['count', 'ratio'])
  print(nan_data.head(10))
  ```

- 统计数据的训练集的偏度，并打印偏度前10位

  ```python
  train = data.loc[data['source'] == "train"]
  test = data.loc[data['sourse'] == "test"]
  train.drop(['source'], axis=1, inplace=True)
  skewness = train.iloc[:, :-1].apply(lambda x: x.shew())
  skewness = skewness.sort_values(ascending=False)
  print(skewness.head(10))
  ```

### 降低数据维度

- 使用特征选择计算每个特征与目标变量的相关系数，保留相关系数大于0.01的特征

  ```python
  corr_target = train.corr()['移动房车险数量']
  important_feature = corr_target[np.abs(corr_target) >= 0.01].index.tolist()
  print(len(important_feature))
  train = train[important_feature]
  test = test[important_feature]
  ```

### 平衡数据集

- 使用重采样方式调整样本比例

  ```python
  from sklearn.utils import resample, shuffle
  train_up = train[train['移动房车险数量'] == 1]
  train_down = train[train['移动房车险数量'] == 0]
  train_up = resample(train_up, n_samples=696, random_state=0)
  train_down = resample(train_down, n_samples=1095, random_state=0)
  train = shuffle(pd.concat([train_up, train_down]))
  ```



## 3. Python机器学习实战案例 -- 基于分类算法的学习失败预警

### 数据探查及特征选择

- 将中文字段进行因子化处理为数字型变量

  ```python
  factor = pd.factorize(df['SEX'])
  df.SEX = factor[0]
  ```

- 查看空值情况，对特征值为空的样本以0填充

  ```python
  null_columns = df.columns[df.isnull().any()]
  print(df[df.isnull().any(axis=1)][null_columns].head())
  df = df.fillna(0)
  ```

- 生成标签列，以60分将学生划分为及格和不及格

  ```python
  df['SState'] = np.where(df['TOTALSCORE'] > 60, 0, 1)
  ```

### 数据集划分及不平衡样本处理

- 如果某一类别下的样本数量超过另一类别的样本量8倍，则对其进行下采样，将下采样之后样本与另一类进行合并，组成一个新的df

  ```python
  df_majority = df[df.SState == 0]
  df_minority = df[df.SState == 1]
  count_times = 8
  df_majority_downsampled = df_majority
  if len(df_majority) > len(df_minority) * count_times:
      new_majority_count = len(df_minority) * count_times
      df_majority_downsampled = resample(df_majority, replace=False, n_sample=new_major_count, random_state=123)
      df = pd.concat([df_majority_downsampled, df_minority])
      df.SState.value_counts()
  ```

- 将整体数据集按照8：2的比例随机划分为训练集和测试集

  ```
  X = df.iloc[:, 0:len(df.columns.tolist()) - 1].values
  y = df.iloc[:, len(df.columns.tolist()) - 1].values
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21)
  ```

### 样本生成及标准化处理

- 采用SMOTE算法通过对小样本数据进行学习以合成新样本

  ```python
  from collections import Counter
  from imblearn.over_sampling import SMOTE
  sm = SMOTE(random_state=42)
  X_res, y_res = sm.fit_resample(X_train, y_train)
  X_train = X_res
  y_train = y_res
  ```

- 对所有输入变量采用sklearn中的StandardScaler标准化方法转换

  ```python
  scaler = StandardScaler()
  X_train = scaler.fit_transform(X_train)
  X_test = scaler.transform(X_test)
  ```



## 4. Python机器学习实战案例 -- 快销行业客户行为分析与流失

### 数据整理

- 将空值填充为0进行替换

  ```python
  df.ActivityKeep = df.AcivityKeep.fillna(0)
  df.IntegralCurrentPoints = df.IntergralCurrentPoints.fillna(0)
  ```

- 将城市进行数字化转换

  ```python
  factor = pd.factorize(df['OrderProvinceCity'])
  df.OrderProvinceCity = factor[0]
  definitions = factor[1]
  ```

### 数据统计与探查



## 5. https://blog.csdn.net/akun1213/article/details/122766741

### 文件合并

```python
all_data = pd.merge(left=file_one,right=file_two,how='outer')
```

### 数据筛选

- 保留需要的数据

### 数据清理

- 检测与处理重复值

  ```python
  all_data[all_data.duplicated().values==True]
  all_data = all_data.drop_duplicates(ignore_index=True)
  all_data.head(10)
  ```

- 处理缺失值，统一单位

  ```python
  male_data = basketball_data[basketball_data['性别'].apply(lambda x:x=='男')]
  male_data = male_data.copy()
  male_height = male_data['身高'].dropna()
  fill_male_height = round(male_height.apply(lambda x:x[0:-2]).astype(int).mean())
  fill_male_height = str(int(fill_male_height))+'厘米'
  male_data.loc[:,'身高'] = male_data.loc[:,'身高'].fillna(fill_male_height)
  male_data.loc[:,'身高'] = male_data.loc[:,'身高'].apply(lambda x:x[0:-2]).astype(int)
  male_data.rename(columns={'身高':'身高/cm'},inplace=True)
  ```

- 用3σ方法检测与处理异常值

  ```python
  import numpy as np
  
  def three_sigma(ser):
      mean_data = ser.mean()
      std_data = ser.std()
      rule = (mean_data-3*std_data>ser) | (mean_data+3*std_data<ser)
      index = np.arange(ser.shape[0])[rule]
      outliers = ser.iloc[index]
      return outliers
  
  female_weight = basketball_data[basketball_data['性别']=='女']
  three_sigma(female_weight['体重/kg'])
  ```

