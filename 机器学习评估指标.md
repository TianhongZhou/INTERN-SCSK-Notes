# 机器学习评估指标

## 混淆矩阵

|           | 实际为A类           | 实际为B类           |
| --------- | ------------------- | ------------------- |
| 预测为A类 | TP - True Positive  | FP - False Positive |
| 预测为B类 | FN - False Negative | TN - True Negative  |

## Accuracy 准确率

- $$
  Accuracy= \frac{\text{correct predictions}}{\text{all predictions}}=\frac{TP+TN}{TP+TN+FP+FN}
  $$

- **预测正确的结果占总样本的百分比**

- 通常情况下，准确率越高，模型越准确

- 准确率可能存在的问题

  - 当我们的数据有2个以上的类时，如3个或更多类，我们可以得到80％的分类准确率，但是我们却不知道是否所有的类别都被预测得同样好，或者说模型是否忽略了一个或两个类
  - 比如在样本集中，正样本有90个，负样本有10个，样本是严重的不均衡。对于这种情况，我们只需要将全部样本预测为正样本，就能得到90%的准确率，但是完全没有意义。对于新数据，完全体现不出准确率。因此，在样本不平衡的情况下，得到的高准确率没有任何意义，此时准确率就会失效

- sklearn中计算准确率示例

  ```python
  import numpy as np
  from sklearn.metrics import accuracy_score
  
  y_pred = [0, 2, 1, 3]
  y_true = [0, 1, 2, 3]
  print(accuracy_score(y_true, y_pred))  # 0.5
  print(accuracy_score(y_true, y_pred, normalize=False))  # 2
  
  # 在具有二元标签指示符的多标签分类案例中
  print(accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))))  # 0.5
  ```

  - 函数接口描述

    ```python
    准确度分类得分
    
    在多标签分类中，此函数计算子集精度：为样本预测的标签集必须完全匹配y_true（实际标签）中相应的标签集
    
    参数
    y_true : 一维数组，或标签指示符 / 稀疏矩阵，实际（正确的）标签
    y_pred : 一维数组，或标签指示符 / 稀疏矩阵，分类器返回的预测标签
    normalize : 布尔值, 可选的(默认为True). 如果为False，返回分类正确的样本数量，否则，返回正确分类的得分
    sample_weight : 形状为[样本数量]的数组，可选 样本权重
    
    返回值
    score : 浮点型
    如果normalize为True，返回正确分类的得分（浮点型），否则返回分类正确的样本数量（整型）
    当normalize为True时，最好的表现是score为1，当normalize为False时，最好的表现是score未样本数量
    ```

## Precision 精确率

- $$
  Precision=\frac{\text{positive predicted correctly}}{\text{all positive predictions}}=\frac{TP}{TP + FP}
  $$

- **所有被预测为正的样本中实际为正的样本的概率**

- 简单来说就是：你认为对的中，有多少确实是对的，所占的比率

- 精准率代表对正样本结果中的预测准确程度

- 比如，瓜农拉来了一车西瓜，这时我们可能不太关心这一车瓜中有多少好瓜，而更关心“自己挑中的瓜（挑中的瓜就是我们预测的好瓜）中有多少好瓜”

- sklearn中计算准确率示例

  ```python
  from sklearn.metrics import precision_score
  
  y_true = [0, 1, 2, 0, 1, 2]
  y_pred = [0, 2, 1, 0, 0, 1]
  print(precision_score(y_true, y_pred, average='macro'))  # 0.2222222222222222
  print(precision_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
  print(precision_score(y_true, y_pred, average='weighted'))  # 0.2222222222222222
  print(precision_score(y_true, y_pred, average=None))  # [0.66666667 0.         0.        ]
  ```

  - 函数接口描述

    ```python
    计算精确率
     
    精确率是 tp / (tp + fp)的比例，其中tp是真正性的数量，fp是假正性的数量. 精确率直观地可以说是分类器不将负样本标记为正样本的能力
     
    精确率最好的值是1，最差的值是0
     
    参数
    y_true : 一维数组，或标签指示符 / 稀疏矩阵，实际（正确的）标签
    y_pred : 一维数组，或标签指示符 / 稀疏矩阵，分类器返回的预测标签
    labels : 列表，可选值. 当average != binary时被包含的标签集合，如果average是None的话还包含它们的顺序. 在数据中存在的标签可以被排除，比如计算一个忽略多数负类的多类平均值时，数据中没有出现的标签会导致宏平均值（marco average）含有0个组件. 对于多标签的目标，标签是列索引. 默认情况下，y_true和y_pred中的所有标签按照排序后的顺序使用
    pos_label : 字符串或整型，默认为1. 如果average = binary并且数据是二进制时需要被报告的类. 若果数据是多类的或者多标签的，这将被忽略；设置labels=[pos_label]和average != binary就只会报告设置的特定标签的分数
    average : 字符串，可选值为[None, ‘binary’ (默认), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]. 多类或 者多标签目标需要这个参数. 如果为None，每个类别的分数将会返回. 否则，它决定了数据的平均值类型
    ‘binary’: 仅报告由pos_label指定的类的结果. 这仅适用于目标（y_{true, pred}）是二进制的情况
    ‘micro’: 通过计算总的真正性、假负性和假正性来全局计算指标
    ‘macro’: 为每个标签计算指标，找到它们未加权的均值. 它不考虑标签数量不平衡的情况
    ‘weighted’: 为每个标签计算指标，并通过各类占比找到它们的加权均值（每个标签的正例数）.它解决了’macro’的标签不平衡问题；它可以产生不在精确率和召回率之间的F-score
    ‘samples’: 为每个实例计算指标，找到它们的均值（只在多标签分类的时候有意义，并且和函数accuracy_score不同）
    sample_weight : 形状为[样本数量]的数组，可选参数. 样本权重
     
    返回值
    precision : 浮点数(如果average不是None) 或浮点数数组, shape =[唯一标签的数量]
    二分类中正类的精确率或者在多分类任务中每个类的精确率的加权平均
    ```

## Recall 召回率

- $$
  Recall=\frac{\text{predicted to be positive}}{\text{all positive observations}}=\frac{TP}{TP+FN}=\frac{TP}{P}
  $$

- **实际为正的样本中被预测为正样本的概率**

- 简单来说就是：本来是对的中，你找回了多少对的，所占的比率

- 召回率越高，代表实际为正样本被预测出来的概率越高

- 精确率和召回率是既矛盾又统一的两个指标，为了提高精确率，分类器需要尽量在“更高把握”时才把样本预测为正样本，但此时往往会因为过于保守而漏掉很多“没有把握”的正样本，导致召回率降低

- sklearn中计算准确率示例

  ```python
  from sklearn.metrics import recall_score
  
  y_true = [0, 1, 2, 0, 1, 2]
  y_pred = [0, 2, 1, 0, 0, 1]
  print(recall_score(y_true, y_pred, average='macro'))  # 0.3333333333333333
  print(recall_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
  print(recall_score(y_true, y_pred, average='weighted'))  # 0.3333333333333333
  print(recall_score(y_true, y_pred, average=None))  # [1. 0. 0.]
  ```

  - 函数接口描述

    ```python
    计算召回率
     
    召回率是比率tp / (tp + fn)，其中tp是真正性的数量，fn是假负性的数量. 召回率直观地说是分类器找到所有正样本的能力
    召回率最好的值是1，最差的值是0
    
    参数与精确率一样
     
    返回值
    recall : 浮点数(如果average不是None) 或者浮点数数组，shape = [唯一标签的数量]
    二分类中正类的召回率或者多分类任务中每个类别召回率的加权平均值
    ```

## F1 分数

- $$
  F_1=2\cdot\frac{\text{Recall}\cdot\text{Precision}}{\text{Recall}+\text{Precision}}
  $$

- **精确率和召回率都越高越好，为了综合精确率和召回率的表现，在两者之间的平衡点**

- 在一些应用中，对精确率和召回率的重视程度有所不同。例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，此时精确率更重要，而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时召回率更重要

- sklearn中计算F1示例

  ```python
  from sklearn.metrics import f1_score
  
  y_true = [0, 1, 2, 0, 1, 2]
  y_pred = [0, 2, 1, 0, 0, 1]
  print(f1_score(y_true, y_pred, average='macro'))  # 0.26666666666666666
  print(f1_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
  print(f1_score(y_true, y_pred, average='weighted'))  # 0.26666666666666666
  print(f1_score(y_true, y_pred, average=None))  # [0.8 0.  0. ]
  ```

  - 函数接口描述

    ```
    计算F1 score，它也被叫做F-score或F-measure
     
    F1 score可以解释为精确率和召回率的加权平均值. F1 score的最好值为1，最差值为0. 精确率和召回率对F1 score的相对贡献是相等的. F1 score的计算公式为：
    F1 = 2 * (precision * recall) / (precision + recall)
     
    参数与精确率一样
     
    在多类别或者多标签的情况下，这是权重取决于average参数的对于每个类别的F1 score的加权平均值
    返回值
    f1_score : 浮点数或者是浮点数数组，shape=[唯一标签的数量]
    二分类中的正类的F1 score或者是多分类任务中每个类别F1 score的加权平均
    ```

- Fβ-Score是F1-Score的一般形式，能够让我们表达出对精确率和召回率的不同偏好，它的定义为

- $$
  F_\beta=(1+\beta^2)\frac{\text{Recall}\cdot\text{Precision}}{\text{Recall}+(\beta^2\cdot\text{Precision})}
  $$

- 当β=1时，精确率和召回率同等重要

- 当β>1时，召回率的重要程度是精确率的β倍

- 当β<1时，召回率的重要程度是精确率的1/β倍

- F1是基于精确率与召回率的调和平均，特点是会更多聚焦在较低的值，对每个指标非常重视

- **不同模型的精确率与召回率的平均越高，模型越优**

- 平均算法

  - 调和平均数：
    $$
    H_n=\frac{n}{\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}}=\frac{2}{\frac{1}{\text{Recall}}+\frac{1}{\text{Precision}}}=2\cdot\frac{\text{Recall}\cdot\text{Precision}}{\text{Recall}+\text{Precision}}=F_1
    $$

  - 几何平均数
    $$
    G_n=(a_1\times a_2\times \cdots\times a_n)^{\frac{1}{n}}=(\text{Recall}\cdot\text{Precission})^{\frac{1}{2}}=\sqrt{\text{Recall}\cdot\text{Precission}} = G
    $$

  - 算术平均数
    $$
    A_n=\frac{a_1+a_2+\cdots+a_n}{n}
    $$

  - 平方平均数
    $$
    Q_n=\sqrt{\frac{a_1^2+a_2^2+\cdots+a_n^2}{n}}
    $$

  - 这四种平均数满足
    $$
    H_n\leq G_n\leq A_n\leq Q_n
    $$

## 示例

模型1预测结果

| 物品     | 预测           | 实际       | 混淆矩阵结果 |
| -------- | -------------- | ---------- | ------------ |
| 包       | 可能是iphone   | 不是iphone | FP           |
| 围巾     | 可能不是iphone | 不是iphone | TN           |
| ipad     | 可能是iphone   | 不是iphone | FP           |
| 纸巾     | 可能不是iphone | 不是iphone | TN           |
| USB      | 可能是iphone   | 不是iphone | FP           |
| 耳机     | 可能不是iphone | 不是iphone | TN           |
| iphone4s | 可能是iphone   | 是iphone   | TP           |
| iphone7  | 可能不是iphone | 是iphone   | FN           |

$$
Accuracy=\frac{TP+TN}{TP+TN+FP+FN}=\frac{4}{8}=50\% \\
Precision =\frac{TP}{TP + FP}=\frac{1}{4}=25\% \\
Recall=\frac{TP}{TP+FN}=\frac{1}{2}=50\% \\
F_1 =2\cdot\frac{\text{Recall}\cdot\text{Precision}}{\text{Recall}+\text{Precision}}=2\cdot\frac{50\%\cdot25\%}{50\%+25\%}=33.3\%\\
G=\sqrt{\text{Recall}\cdot\text{Precission}}=\sqrt{50\%\cdot25\%}=35.4\%
$$

模型2预测结果

| 物品     | 预测           | 实际       | 混淆矩阵结果 |
| -------- | -------------- | ---------- | ------------ |
| 包       | 可能不是iphone | 不是iphone | TN           |
| 围巾     | 可能不是iphone | 不是iphone | TN           |
| ipad     | 可能是iphone   | 不是iphone | FP           |
| 纸巾     | 可能不是iphone | 不是iphone | TN           |
| USB      | 可能是iphone   | 不是iphone | FP           |
| 耳机     | 可能不是iphone | 不是iphone | TN           |
| iphone4s | 可能是iphone   | 是iphone   | TP           |
| iphone7  | 可能不是iphone | 是iphone   | FN           |

$$
Accuracy=\frac{TP+TN}{TP+TN+FP+FN}=\frac{5}{8}=62.5\% \\
Precision =\frac{TP}{TP + FP}=\frac{1}{3}=33.3\% \\
Recall=\frac{TP}{TP+FN}=\frac{1}{2}=50\% \\
F_1 =2\cdot\frac{\text{Recall}\cdot\text{Precision}}{\text{Recall}+\text{Precision}}=2\cdot\frac{50\%\cdot33.3\%}{50\%+33.3\%}=40\%\\
G=\sqrt{\text{Recall}\cdot\text{Precission}}=\sqrt{50\%\cdot33.3\%}=40.8\%
$$

- 因为模型2的F1与G分数都模型1高，所以模型2优与模型1

## ROC

- 真正利率 TPR

  - 预测为正例且实际为正例的样本占所有正例样本的比例

  $$
  TPR=\frac{TP}{TP+FN}
  $$

- 假正例率 FPR

  - 预测为正例但实际为负例的样本占所有负例样本的比例

  $$
  FPR=\frac{FP}{FP+TN}
  $$

- 根据模型的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行检测，每次计算出TPR和FPR的值，以FPR为横坐标、TPR为纵坐标作图，就得到了ROC曲线

- 若一个模型的ROC曲线被另一个模型的曲线完全包住，则可断言后者的性能优于前者

- 若两个模型的ROC曲线发生交叉，则难以一般性断言两者的优劣

## AUC

- 指ROC曲线下的面积大小
- 判断分类器（预测模型）优劣的标准：
  - AUC=1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器
  - 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值
  - AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值
  - AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测

